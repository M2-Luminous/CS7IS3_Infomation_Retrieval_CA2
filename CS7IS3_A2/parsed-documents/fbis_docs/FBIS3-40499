FBIS3-40499 "jpjst002___94002" JPRS-JST-94-002 Document Type:JPRS Document Title:Science & Technology Japan Selections from the Proceedings of the 9th NAL Symposium on Aircraft Computational Aerodynamics 7 February 1994 Selections from the Proceedings of the 9th NAL Symposium on Aircraft Computational Aerodynamics Numerical Wind Tunnel; Requirements on the Outline 93FE0499B Tokyo SPECIAL PUBLICATION OF NAT'L AEROSPACE LABORATORY in Japanese Dec 92 pp 91-97 93FE0499B Tokyo SPECIAL PUBLICATION OF NAT'L AEROSPACE LABORATORY Language: Japanese Article Type:CSO [Article by Hajime Miyoshi of the National Aerospace Laboratory] [Text] Abstract: In this paper, it is shown that the Numerical Wind Tunnel (NWT) with an actual performance of more than 100 times higher than Fujitsu-VP400 is feasible using a multi-computer architecture and crossbar interconnection network. 1. Introduction Let me tell you what I did at last year's 3d aircraft computational aerodynamic symposium. I proposed: (R1) That it is necessary to quickly develop a high-speed computer which will exhibit at least 100 times the CFD program processing speed of the Fujitsu-VP400 (hereinafter called simply the VP400), with a main memory area of 32 GB or so for data storage, and to use this in the NAL computer system. This would be for the purpose of exploiting the merits of numerical simulation technology, for the purpose of employing the CFD gains made thus far in the aircraft aerodynamic development which is beginning to be done in Japan, and to push CFD to even higher levels. I also showed that it is possible to quickly implement this computer by adopting parallel computation techniques.[.sup]1[/] In this paper I press last year's argument even further and report on: (1) the considerations that form our starting point for studying the idea of a numerical wind tunnel, (2) the conditions necessary to produce a practical computer exhibiting the performance noted in (R1), (3) the hardware configuration for a computer exhibiting the performance noted in (R1), and (4) a summary of the functions required in program writing and the compiler. Furthermore, with the purposes for developing a computer that will satisfy the performance demands noted in (R1) firmly in mind, in the remainder of this paper I consider that computer to be a numerical wind tunnel (abbreviated NWT). 2. NWT Concept Starting Point In this section I will briefly discuss some aspects of the starting point for the NWT concept. The details are discussed in the References.[.sup]2[/] 2.1 NWT Approach (1) NWT Will Be Distributed Main Memory Type Parallel Computer In terms of how the main memory is structured, we may divide parallel computers into those which employ common main memory and those which employ distributed main memory (cf. Figure 1). We decided to make the NWT a distributed main memory parallel computer for three reasons, namely (1) the fact that there are few limitations on improving raw computer processing speeds, (2) the fact that there are great possibilities for improving performance in the future by incorporating advances in element technology, and (3) the fact that the big problem with distributed main memory type parallel computers -- i.e. the problem of addressing main memory -- can be to some extent resolved, we now think, by incorporating virtual common main memory space.[.sup]3[/] Figure 1. Distributed Main Memory Parallel (2) Element Computer (abbreviated PE) Will Be Vector Computer Equipped With Vector Registers (abbreviated VR) We have learned from our experience to date with vector computers that VR-equipped computers having chaining functions are suitable for CFD programming. The Fortran compiler technology for this mode of computation has reached a high level, moreover, and so long as the hardware has the standard hardware functions for vector processing, the vectoring ratio for CFD programs is 99% or thereabouts. That is why we decided that the PE would be a VR-equipped vector computer. 2.2 Preconditions for CFD Programs Paralleling and Vectoring Ratios Reaches 99% and Higher in CFD Programs. Most of NAL's CFD programs now attain a vectoring ratio of 99% or higher. If the degree of paralleling is determined in line with the scale of the problem, the paralleling ratio of CFD programs also reaches 99% and above. Both of these ratios rise as the scale of the problem increases relative to the degree of paralleling. 2.3 Usable Hardware Elements (1) Theoretical Elements Recent trends in theoretical element (device) speeds are plotted in Figure 2. The gate switching delay time is plotted on the vertical axis and the year on the horizontal axis. In Figure 3 are plotted the trends in theoretical element densities, with gate density per chip plotted on the vertical axis and the year on the horizontal axis. Based on these two figures, and on recent theoretical element per-chip power consumption trends, we expect that the following three devices can all be used as NWT devices. Figure 2. Theoretical Element Speed Trends Figure 3. Theoretical Element Density Trends (i) 30,000 gates/chip: Switching delay time 50 ps (10[.sup]-12[/] seconds); Power consumption 40 W/chip ECL gate array (ii) 100,000 gates/chip: Switching delay time 350 ps (10[.sup]-12[/] seconds); Power consumption 20 W/chip BiCMOS gate array (iii) 200,000 gates/chip: Switching delay time 600 ps (10[.sup]-12[/] seconds); Power consumption 10 W/chip CMOS gate array (2) Main Memory Elements As to the main memory element, the 4 M/chip SRAM is beginning to be mass-produced. Accordingly, it is possible to use either a 4 M/chip or 1 M/chip SRAM as the main memory element and implement a main memory of 30 to 40 GB.[.sup]1[/] Which chip to go with will depend on cost and speed considerations. 3. Conditions Needed for Implementing Numerical Wind Tunnel In the introduction we noted that the performance goals for the numerical wind tunnel have been established, but the answers forthcoming from computer engineering do not give a clear picture of how this can be implemented. There are many technical decisions which have to be made. We discuss briefly here some conditions that will provide judgment criteria when making technical decisions. 3.1 NWT Cost When we look at the numerical wind tunnel from the perspective of R&D on aerodynamic technology for aircraft, the numerical wind tunnel is a piece of aerodynamic testing equipment that complements the conventional wind tunnel. If the cost is too high, therefore, the NWT cannot be used in parametric studies and the like for the purpose of doing aerodynamic design of aircraft utilizing the advantages of computers. Accordingly, if we are to implement the NWT, then we must look beyond performance alone, and do our best to lower development and production costs and operating costs. When considering such costs, naturally we must think about the cost per data set of each wind tunnel test. In this day and age we must consider the cost of data as well as the reliability of data for numerical simulations based on the RANS equations. This means that it will be very difficult to implement the NWT by linking up dozens of supercomputers. 3.2 Reliability The improvements in recent years in both element technology and mounting technology have had an enormous impact in enhancing the reliability of computers. It is nevertheless a well known fact that devices adopted for improved reliability often have a negative impact on performance improvement and cost reduction. With a numerical wind tunnel, the volume of materials used will probably become enormous, but what would be appropriate for the purpose of improving reliability and maintainability? The users of the numerical wind tunnel will be CFD researchers and technicians, rather than computer engineering specialists, so a stringent attitude will probably be taken towards malfunctions. Furthermore, since the numerical wind tunnel is not just an experimental computer but also a practical computer, it will be just as important -- in terms of the success or failure of the numerical wind tunnel -- to maintain NWT hardware reliability at high levels as to achieve the targeted performance in the first place. 3.3 Performance Demands for Element Computer Two items in particular will be necessary in order to take the NWT, which is a distributed main memory parallel computer, and achieve its widespread use among ordinary operators who are not computer specialists, as follows. (i) Either an optimizing compiler for Fortran programs written according to a Fortran standard for parallel programming, or a compiler that automatically optimizes programs written in current Fortran for parallel implementation. (ii) Parallel-running programming support software including debugging and tuning tools. We will bring a practical computer in among CFD technicians and researchers who are not computer specialists and seek to devise both (i) and (ii) above in conversations with computer specialists and ordinary users. So doing, we will adopt a policy of promoting the growth and proliferation of knowledge needed for high-level utilization of parallel computers among users. When this policy is adopted, it is necessary to guarantee the following minimum limitation for users. (R2) The processing of jobs that are appropriate for processing using the VP400 are to be processed on one element computer without program alteration, and the processing speed therefore is to be the VP400 speed or higher. 3.4 Conditions for Interconnecting Network -- Compatibility Between Interconnecting Network and CFD Computation Methods When we survey recent CFD computation techniques from the perspective of parallel computing, we see the following. (1) Computation schemes are tending to become more complex to achieve higher precision. (2) Research is being done on non-structure lattices as well as on structure lattices. (3) In terms of solving methods, negative solution methods in general, and the IAF method in particular, are dominant over positive solving methods, but if interest shifts to problems such as unsteady problems where there is not much of an advantage with the negative solving methods, the present situation could change. Nevertheless, when it comes to applications to actual problems of technological development, the track record of the IAF method cannot be disregarded. Item (1) will make stringent demands on the performance of parallel computers in the area of interconnecting network data transfers. In processing CFD programs which use the non-structure lattices noted in item (2), list vector processing will be mandatory. This will make stringent demands not only on interconnecting network data transfer performance, but also on hardware and software performance. In methods for solving linear equations that are negative solving methods, it will be necessary to adopt sequential computation for single-axis directions in three-dimensional space. With structure lattices which are graphically simplified, it is not practicable to disregard the computation methods that employ sequential computations in single-axis directions, so, when one axis is used in vectoring, the remaining axis will be parallelized. When we consider that the scale of problems visualized for the NWT having the power set forth in (R1) is on the order of several thousand x several hundred x several hundred, this comes bouncing back in the form of a limitation on the number of parallel computers. In particular, with the IAF technique that is an important negative solution method, the sequential computation axis changes from X to Y to Z, so not only are stringent performance demands made on interconnecting network data transfers, but stringent demands are also made on the network topology and on the number of element computers configuring the numerical wind tunnel. Our numerical wind tunnel must be able to deal effectively with all CFD techniques. And we should not adopt an interconnecting network which exhibits high performance only for specific CFD techniques and inadequate performance for other techniques. From (R2) set forth in 3.3 above and the conditions noted in this subsection, we can see that it will be very difficult to implement NWT with several thousands to several tens of thousands of microprocessors. 4. NWT Hardware Configuration, Performance 4.1 PE Configuration, Performance The processing speed of a VR-equipped vector computer is determined mainly by the following eight factors. (i) Machine clock cycle time (hereinafter abbreviated _t_) (ii) Pipeline multiplicity (iii) Type and number of pipelines, and number of pipelines supporting simultaneous activity (iv) Ability of main memory to provide data (v) VR number and length (vi) Rise time for various pipelines (vii) Instruction execution control format (viii) Scalar instruction processing mode, register configuration, cache memory volume and control mode. First, we assumed that items (vii) and (viii) would be the same as in the VP400, and then prepared many PE candidate models in terms of PE design parameters for the hardware resource quantities and timing noted in items (ii) through (vi). Next we examined typical CFD programs at NAL and selected 18 representative characteristics including number of vector instructions, ratio of various types of vector instruction, and the ratio between vector load/store instructions and vector operation instructions and then estimated 18 DO-loop processing speeds for various PE models, with vector lengths of 32 and 128, using the software simulator VTAP which simulates the actions of a vector computer. The error in the VTAP simulations for the VP400 processing speeds for the 18 DO loops was within +/- 10% and the mean error was 6.8%. In Figure 4 we have plotted the results for one PE model series. The pipeline multiplicity in this PE model series was varied from 1 to 2 to 4 to 8 to 16, with the other parameters being made up of six fixed models. Figure 4. PE Pipeline Multiplicity (128 VRs) Pipeline multiplicity is plotted on the horizontal axis in this figure, and machine clock time _t_ on the vertical axis. Here we modify the (R2) demand that the PE CFD program processing speed be VP400 or better to (R2'). (R2') VTAP estimated value for PE processing speed for all 18 DO loops at vector lengths of 32 and 128 > VTAP estimated values for VP400 processing speed. The times in Figure 4 of 1.6, 3.1, 6.4, 12.6, and 12.9 ns are the required PE machine clock times as calculated from the simulation results for maximum machine clock times at which the PE model satisfies (R2'), and then shortened another 10% in the interest of safety. The ECL and BiCMOS in the diagram are the names of the logic elements that should be adopted to achieve these machine clock times. The x symbol indicates that it is difficult to achieve this value of _t_ by means of usable logic elements. After doing comparative studies on the results of simulations on various other PE models (besides this PE model series), we found that this PE model series is the most effective series among the PE models which we studied. After comparing the power consumption of a PE with _t_ = 3.1 ns and a pipeline multiplicity of 2 using ECL elements with that of a PE with _t_ = 12.6 ns and pipeline multiplicity of 8 using BiCMOS elements, we found that the latter PE model would be better for use in the NWT. When BiCMOS elements are used, moreover, one can be sure that a machine clock time of around 10 ns will be achieved. In Table 1 [not reproduced] are represented the processing times (in MFLOPS) estimated by VTAP for two NAL CFD programs (NS3D and NSMD) for this PE model, and the actual measured MFLOPS values and VTAP-estimated MFLOPS values for the VP400 and VP200. From this table we see that we can expect a CFD program processing speed for the NWT PE that is 1.4 times the VP400 speed. 4.2 NWT Hardware Configuration, Performance Since we made it clear in 4.1 above that the PE performance would be about 1.4 times that of the VP400, it is evident that a NWT which satisfies (R1) can be configured with 200 or fewer PE units. If the number of PE units is less than 200, then we are able to adopt an interconnect network that is based either on crossbar interconnection or complete interconnection. By adopting a powerful network, we can expect a PE efficiency of around 0.5. In such a case, the following would apply. -- In order to satisfy R1, it is only necessary to have 150 or more PE units. If we have 256 MB main memory capacity for each PE unit, then the total main memory capacity for the NWT would be about 38 GB. In Figure 5 is presented a conceptual block diagram of the NWT. Figure 5. NWT Conceptual Block Diagram 5. Features Needed in NWT Programming and Compiler (P&C) In this section we give a broad overview of the main P&C features required for maintaining an NWT PE efficiency of 0.5 or better. These features may be implemented in program writing (the programmer's responsibility) or by the compiler, or by a combination of both. In Figures 6-1 and 6-2 are given conceptual running diagrams when programs are run on one PE and on n PE units, respectively. Figure 6-1. Conceptual Diagram of Executing Job With 1 PE Figure 6-2. Conceptual Diagram of Executing Job With n PE S[.sup]i[/]: Execution time required for i'th sequential process in program SO[.sup]i[/]: OS execution time required for S[.sup]i[/] process SI[.sup]i[/]: OS execution time for I/O processing required for S[.sup]i[/] process I/O[.sup]i[/]: I/O Execution time required for S[.sup]i[/] process P[.sup]j[/]: Execution time when j'th parallel-processable portion in program processed by 1 PE PO[.sup]j[/]: OS execution time required for P[.sup]j[/] process PI[.sup]j[/]: OS execution time for I/O processing required for P[.sup]j[/] process I/O[.sup]j[/]: I/O execution time required for P[.sup]j[/] process ParaOS[.sup]i[/]: OS processing time that becomes necessary to do parallel processing P[.sup]j1[/]: When parallel-processing P[.sub]j[/], execution time for P[.sup]j[/] portion that should be processed by PE[.sup]1[/] (i.e. by the 1st PE) S/RO[.sup]j1[/]: OS execution time required to S/R (data transfer) data needed for P[.sup]j1[/] process S/R[.sup]j1[/]: Time required to send data PO[.sup]j1[/]: Execution time for OS processing required for P[.sup]j1[/] process PI[.sup]j1[/]: OS execution time for I/O processing required for P[.sup]j1[/] process I/O[.sup]j1[/]: I/O execution time necessary to execute P[.sup]j1[/] SYO[.sup]j1[/]: Execution for OS processing required to synchronize PE[.sup]1[/] with other PEs W[.sup]j1[/]: PE[.sub]1[/] synchronization wait time Equations [Text] equals Execution time when P[.sup]j[/] is parallel-processed The time (TS) for processing the program with one PE unit and the time (TPN) for processing the program with n PE units are given by the following formulas. Now, if we take F as the number of floating point operations (included only in P[.sup]j[/] or S[.sub]i[/]) required to execute the entire program, we get F/TS as the program processing speed for one PE unit, and (F/TS) x (TS/TPN) as the program processing speed with n PE units. Accordingly, it is essential to make TS/TPN as large as possible in the interest of improving speed by parallel processing. The P&C features needed, in conceptual terms, to accomplish are evident from Figures 6-1 and 6-2. The most important ones are noted below. (A) Improved ability of P&C to find those portions that are parallel-processable so as to shorten S[.sup]i[/] and reduce the ParaOS[.sup]i[/] frequency (B) Improved P&C capabilities to facilitate reduction in data transmission volume and frequency so as to reduce frequency of S\R[.sup]j1[/] and shorten processing time (C) P&C functions to facilitate P[.sup]j1[/] and S\R[.sup]j1[/] parallel action (D) P&C functions to facilitate PI[.sup]j1[/] and P[.sup]j1[/] parallel actions (E) P&C functions to facilitate reduction of volume and frequency of referencing data stored in other PEs for the purpose of reducing the SYO[.sup]j1[/] frequency (F) In the interest of decreasing W[.sup]j1[/] as much as possible, (i) P&C functions to facilitate equalization of divisions of P[.sup]j[/] to P[.sub]j1[/] (ii) Since it is necessary to make it so that S\R[.sup]j1[/] processing does not compete on the network more than the absolute minimum, in order to make S\R[.sup]j1[/] equal for all 1's, the necessary P&C scheduling functions There must naturally be cooperation by both the hardware and the computation methods to facilitate the achievement of (A) through (F). If the degree to which (A) through (F) are achieved rises, it will be possible to satisfy (R1) with our NWT CFD program processing speed using P&C. References 1. Miyoshi, H.: "CFD suishin ni hitsuyona keisanki seino," NAL SP-13, pp 1-26, September, 1990. 2. Miyoshi, H.: "Kogiken Chokosoku suchi fudo (UHSNWT) no koso," NAL TR-1108, May, 1991. 3. Okada, S., and Takamura, M.: "CFD muke heiretsu keisanki no sofutouea," NAL SP-13, pp 109-116, September, 1990.
