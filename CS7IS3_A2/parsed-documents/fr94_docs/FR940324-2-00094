FR940324-2-00094 FR940324-2-00044 Process Evaluation Process evaluation is a quantitative and qualitative description of a project that documents its evolution from inception through implementation and completion. The purpose of the process evaluation is to document what happened in the project and what was learned, what barriers existed to inhibit implementation, what was done to overcome these barriers, and what should be done differently in future projects. A process evaluation should also document the context for the project, that is, how the program fit into the community and interacted with the existing relevant human service programs and resources that were available to serve the target population. The use of resources and the acceptability and appropriateness of the program activities for the target population to meet the project objectives should also be addressed. Formal and on-going linkages between project objectives and program interventions should be developed to ensure feedback and appropriate project modifications. Outcome Evaluation The purpose of the outcome evaluation is to determine the effectiveness of the intervention, and applicants must propose an evaluation design which will allow this determination to be made. Outcome evaluation assesses whether the project was effective in achieving its goals, objectives and activities. The plan to generate, process and analyze data should be detailed and clearly articulated. For the purposes of this grant program, the outcome evaluation must: (a) Identify the specific factors to be addressed according to the terms of the model selected to conceptualize the program; (b) Specify the proposed interventions to be implemented detailing frequency and intensity of exposure of each intervention proposed, per member of target population; (c) Specify the desired outcomes in relation to both the identified factors (generally these are intermediate outcome measures), and the selected interventions; (d) Identify the instruments for measuring the factors to be addressed and for monitoring changes related to desired outcomes and submit copies of any non-standard instruments; (e) Specify the indicators of ATOD use to be monitored; (f) Identify the instruments to be used for gathering those measurements related to ATOD indicators; (g) Describe the plan for data collection, data processing and data analysis. The outcome evaluation design should be as rigorous as possible. Whenever feasible, the outcome evaluation design should employ a time series design that allows for comparisons within and between control or matched comparison groups. For capturing changes at the community level, a pre-post with repeated measures design, for community indicators, is acceptable. For measuring changes at the individual/group level, control or comparison groups, utilizing either random assignment or matched comparison, should be used. However, if an applicant chooses another design without the use of a randomized control or matched comparison group, the applicant must describe the alternate design in detail and explain the method for determining the effectiveness of the intervention. If the target group(s) is to be randomly selected, indicate the method for randomization of target and control group. If a matched comparison group is to be used, detail the method for matching the groups. In all cases, applicants must include a discussion of plans to deal with attrition, accretion, and other threats to internal and external validity. All applicants must also collect baseline data, yearly progress report data, and end of project data on all indices. In accordance with the age guidelines provided in the Modules, these data should be designed to answer questions that include, but are not limited to, the following: (Additional questions specific to each priority area are included in the Recommended Approach section of each module).
