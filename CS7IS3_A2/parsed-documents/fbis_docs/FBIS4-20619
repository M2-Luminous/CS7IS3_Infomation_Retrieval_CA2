FBIS4-20619 "jpjst012__l94052" JPRS-JST-94-012-L JPRS Science & Technology Japan 14 April 1994 Automotive Technologies Toyota Automated Highway Vehicle System Toyota Automated Highway Vehicle System 43070048A Tokyo TOYOTA TECHNICAL REVIEW in English Sep 93 pp 18-24 -- FOR OFFICIAL USE ONLY 43070048A Tokyo TOYOTA TECHNICAL REVIEW English CSO [Article by Akihide Tachibana and Keji Aoki, Research & Development Div. III] [Text] An Automated Highway Vehicle System using a computer vision system has been developed and some test running has been executed by the prototype vehicle. This paper describes a system architecture, a white line detection method, a lateral control method, a longitudinal control method, results of experiment and technical trends. The white line detection method has high accuracy and robustness to environmental conditions by using line edge extraction, a road knowledge model and electric shutter control. A lateral control method makes use of preview information (a front white line position data) and is able to execute a stable automated running at 60 km/h. A longitudinal control method is able to execute an accurate and smooth following running by driving force control method and a front vehicle detection method. 1. Introduction To realize the safer and more pleasant driving, various efforts have been made in developing intelligent vehicles. To cope with the recent large scale and complex problems of the safety, traffic jams, and other transportation and environmental protection problems, it is thought that the higher level intelligence will be required in the future. As one of the directions toward this objective, an automated vehicle system is drawing attention and vigorous research and development are underway (Table 1). With these in the background, we have begun the development of automated vehicle driving system in assumption of automated driving on the exclusive lane of a highway as we thought that it is the closest to the operational application of the automated driving. Functional requirements for the automated driving include a lateral control (lane keeping), longitudinal control, collision avoidance control, etc. and various methods are proposed for realizing these functions. Particularly, the running lane detection is the nucleus of the automated driving technology. Table 2 indicates the characteristics of the principal detection methods. Of these methods, the vision method, for example, has disadvantages in the processing speed and environment resistance (rain, night, etc.) compared with other methods. Yet, as in the case of a man who drives by watching some meters ahead, this method enables a smooth and stable lateral control since it can obtain preview information. In addition, using the preview information, the vision method can provide the front vehicle and obstacle detection functions which are important for the longitudinal control and collision avoidance control. In the case of other methods, completely different means are needed, however. Thus the preview information is the most important characteristic of the vision method, whose development to a system of higher performance can be expected with the application of recognition technology of road geometry, vehicle, and road sign. From the reason stated above, we have developed the automated vehicle system that applies the vision (white line) information. This paper reports on the system architecture, lateral control method, and the longitudinal control method. Table 1. Development Examples of Automated Vehicle System System Principal of Lane detection Driving speed development method Platooning University of Detection of Special road, California-Berkeley magnetic markers over 100 km/h with a magnetometer Convoy-pilot VW Side wall Special road, detection using over 100 km/h the laser PVS Nissan White line Proving ground detection by vision MOVER-2 Mazda White line Proving ground detection by vision VaMoRs Universitat der White line and Expressway Bundeswehr Munchen road boundary detection by vision ----------------------------------------------------------------------------- |Table 2. Lane Detection Method | ----------------------------------------------------------------------------- |Item/Method |Vision |Magnet |Side wall | ----------------------------------------------------------------------------- |Preview informati-|E |_D_ |x | |on | | | | ----------------------------------------------------------------------------- |Obstacle detection|o |x |x | ----------------------------------------------------------------------------- |Processing speed |_D_ |o |o | ----------------------------------------------------------------------------- |Environment resis-|x |o |o | |tance | | | | ----------------------------------------------------------------------------- |Possibility of re-|o |_D_ |o | |duction in cost | | | | ----------------------------------------------------------------------------- |Possibility of sy-|o |_D_ |x | |stem development | | | | ----------------------------------------------------------------------------- |E: excellent; o: good; _D_: acceptable; x: not acceptable | ----------------------------------------------------------------------------- 2. System Architecture Photo 1 [not reproduced] shows a prototype vehicle for the automated vehicle system. Using a Crown Majesta for the base, this vehicle is installed with the hardware shown in Figure 1. Figure 1. System Architecture 2.1 Computer A computer (hereinafter referred to as ECU: Electronic Control Unit) is arranged for each device for the host, image processing, actuator control, and the sensor processing. With this architecture, the system has attained the real time operability through the load reduction from the host ECU, and the fail-safe measures through the mutual supervision of ECUs. 2.2 Image Processing System The image processing system consists of a CCD camera and the white line detection device. It outputs the white line data for lateral control. 2.2.1 CCD Camera A compact 2/3 inch CCD camera is installed to the left side of the inside rear view mirror. It has the lens focusing distance of 10mm and is mounted at a depression angle of about 24 degrees in consideration of the effect from backlighting and the vehicle pitching and the white line detection area. 2.2.2 White Line Detection Device While various methods are proposed for the white line detection, we have adopted a line edge extraction method for the present system in consideration of the installability to the vehicle and reliability. The white line detection under this method consists of the following two processing: (1) Edge extraction processing The monochromatic luminance data input from the CCD camera is differentiated using a method called Sobel operator, and the points having a sharp variation in luminance (for example, the borderline between the white line and the road surface) in succession is extracted as the edge. This is processed by the special hardware at a high speed. (2) White line recognition processing In the preprocessing, the edges of the roadside guardrail and the buildings are extracted in addition to the white line. To select the correct white line from this list of line segments, the following knowledge concerning the white line is used: - The detection position of the white line continuously changes. - The road curvature does not change sharply. - The white line segment is amply long compared with other false line segments. Through the above-mentioned processing, the white lines on the right and left hand sides as shown in Photo 2 [not reproduced] are detected. The processing time required from the pickup of the image to the output of data is about 300 ms. The white line position data is output to the host ECU at every 100 ms through the parallel image processing using three sheets of the board, however, and the lateral control is executed accordingly. However, since the brightness of the road surface varies by the weather, time zone, shade, etc. during the actual running, the electronic shutter speed of the CCD camera is controlled according to the luminance from the central portion of the running lane to maintain the robustness to the brightness. 2.3 Steering Drive System The steering drive system consists of a DC motor, ECU, steering angle sensor, etc. as shown in Figure 2. Figure 2. Steering Driving System 2.3.1 Steering Actuator The steering actuator consists of an electronic clutch and the reduction gear beside a 3 phase 12 pole brushless DC motor. The driving performance of the main shaft portion is as follows: - Maximum rotation speed: 80 rpm - Maximum steering effort: 3.53 N-m - Minimum steering angle: 1.67 degrees 2.3.2 Steering ECU The steering ECU drives the DC motor according to the target steering angle input from the host ECU. It monitors the steering angle sensor to control the absolute steering angle and one steering amount. In addition, it turns the electronic clutch ON-OFF according to the instruction from the host ECU. 2.4 Longitudinal Distance Sensor A scanning type laser radar is installed in the radiator grille. It has a scanning angle of 300R (minimum curve of highway) and is set for detecting front vehicle running 100 m ahead. - Detection distance: 5 to 120 m - Scanning range: +/- 15 degrees (horizontal direction) - Beam vertical angle: +/- 0.5 degree (vertical direction) - Scanning time: 120 ms/scan 3. Vehicle Control Algorithm This section describes the lateral control method, longitudinal control method, and the front vehicle detection method for vehicle control. 3.1 Lateral Control Method In the lateral control, compatibility is demanded between the high precision lane keeping performance and the direction keeping performance to a target lane. Several control methods have been proposed concerning the steering control.[.sup]1,2[/] We have examined a linear predictable correction model as shown in Figure 3 in consideration of the control performance[.sup]3[/] of the various control models and the capacity of CPU used. To simplify the control system, the steering drive system has been structured to respond promptly to the vehicle dynamics and that with the servo mechanism functioning independently to a target steering angle. Consequently, for the control system, a PD controller to compensate for the deviation (lateral deviation) and the direction (differential of lateral deviation) to the target lane as shown with equation (1): _th_ = K(L, V)e[.sub]t[/] + G(L, V)(e[.sub]t[/]-e[.sub]t-1[/]) (1) Where: _th_ = Target steering angle L = Preview distance V = Vehicle speed K(L, V) = Proportional gain G(L, V) = Differential gain e[.sub]t[/], e[.sub]t-1[/] = Lateral deviation Figure 3. Steering Control Model 3.2 Longitudinal Control Method Important things in the lateral and the collision avoidance control are correct detection of front vehicle on the own running lane and the precision maintenance of safe longitudinal distance from it for the purpose of maintenance of the safety and the improvement of traffic efficiency. 3.2.1 Front Vehicle Detection Method To correctly detect the front vehicle and the collision avoidance object on the own lane, the detection accuracy has been improved by combining the scan type laser radar with images. First, the own running lane (white line) as obtained from the images is converted to the plane coordinates.[.sup]4[/] And if a point presumed to be a vehicle from the distance data and the horizontal angle data from the laser radar should exist inside the own lane on the plane coordinates, it is judged to be the front vehicle. Figure 4 shows the front vehicle detection by converting the white line data on the image picked up from the curve of 150R to the plane coordinates on which the distance data is laid. In this case, the lane detection error was maximum 1.2 m in the longitudinal direction and 0.2 m in the lateral direction. Figure 4. Front Vehicle Detection 3.2.2 Longitudinal Control Method To maintain a longitudinal control correctly, a driving force control method using the vehicle model was examined. As shown in equation (2), the target driving force is calculated from the target longitudinal distance, real longitudinal distance, closed relative velocity, etc., and the throttle and brake are controlled according to the value of driving force. F = M(K(D-D[.sub]t[/])-GV[.sub]r[/]) (2) F � Mg[.sub]0[/] throttle control F < Mg[.sub]0[/] brake control Where: F = Target driving force M = Vehicle mass L = Proportional gain G = Differential gain D = Real longitudinal distance D[.sub]t[/] = Target longitudinal distance V[.sub]r[/] = Closed relative velocity g[.sub]0[/] = Coastdown deceleration 4. Running Test 4.1 Result of Lateral Control Running tests were conducted on the test course including curves of minimum 80R to examine the characteristics of respective parameters, and the target control constants were set up. 4.1.1 Setting of Parameters First the basic examination was conducted using the simulation of simple vehicle model on the preview distance L and the control gain. The control gain was set so that the lateral deviation e from the target lane in the straight running will be the minimum value. When the vehicle passed the curve of 80R under the same gain, the lane keeping performance was maintained while the preview distance L = 15 to 30 m, but when L = under 15 m, the lane keeping performance was understeer, and when L = over 30 m, the same performance was oversteer. Using the above results as precondition, a running test was conducted at 60 km/h to obtain the target preview distance L and the control gain. The preview distance L was set for 10, 20, and 30 m. On each preview distance, the gain was set so that the lateral deviation e on the straight course and the curve (80R) will be a minimum and the vehicle was run under the automated steering. Figures 5 and 6 show the result of the testing from which the following matters have been understood on the preview distance L and the control gain: - When L is made greater, the proportional gain becomes smaller, and the stability increases. - The proportional gains for the straight line and the curve differ. When L is small, compensation is required by detecting the road curvature. When L = 20 to 30 m, a stable running performance can be maintained with a fixed gain. - The greater L, the faster the entry of the lane data. For example, steering starts before entering the curve and the lane is deviated toward the inner wheel. When L is under 25 m, the lane keeping performance is maintained. Figure 5. Preview Distance and Lane Keeping Figure 6. Control Gain Judging from the above, the preview distance L should be set at 20 to 25 m to maintain the steering performance (lane keeping performance and stability). On the other hand, the preview distance should be set as close to the vehicle as possible in the white line detection. This is for preventing the detection error due to the shielding of white line by the front vehicle when the preview distance is long, reflection of the sun light on the road surfaces, and the thinning of the white line on the image. From the above, the preview distance L is set to 20 m for the present system. Then the control gain is set again by giving consideration to the balance of the running performance on the straight course and the curve. 4.1.2 Evaluation of Steering Performance Figure 7 shows the result of automated running at 60 km/h by using the preview distance and the control gain as determined in the preceding section. When compared with the manual steering (Figure 8) for checking the steering control performance, it was found that the fluctuation of lateral deviation e was within 20 cm for both automatic and manual steering. This indicated that the automatic steering has a fairly high level of lane keeping performance. When the yaw rate fluctuation cycle was checked, however, the cycle for the automatic steering was short. It was only several seconds to some 15 seconds for the manual steering. And the stability was somewhat lowered. This was caused because the steering actuator was of the step driving type which was unable to make minute steering and required more frequent steering correction. This fact is reflected in the steering fluctuation of these two modes of steering. The fluctuation of manual steering was +/- 2 degrees while that of the automatic steering was +/- 4 degrees. Thus, although the automatic steering was slightly inferior to the manual steering in the aspect of stability, it achieved such steering performance as the lane keeping, which was close to the manual steering, by using a comparatively simple linear predictable correction model. When the driving speed rises, however, time lag from the image processing adversely affects the automatic steering, degrading the lane keeping performance. This shortcoming may be coped with to a certain extent by adjusting the preview distance and/or the control gain. It is imperative for the automatic steering system to increase the image processing speed to realize stable driving at 100 km/h, however. Figure 7. Automatic Drive Figure 8. Manual Drive 4.2 Result of Longitudinal Control Figure 9 shows the result of following running when the front vehicle is driven at 30 to 40 km/h. When the acceleration/deceleration of the front vehicle was under +/- 0.2 G, it was possible to maintain the longitudinal control with about +/- 1 m error. When the acceleration/deceleration became greater, however, detection error occurred because of the change of the laser radar beam direction due to the pitching of vehicle body or the target performance failed to be accomplished because of the delay of response from the throttle and the brake actuator. Figure 9. Following Running Regarding the front vehicle detection, the front vehicle on the own lane was correctly detected on the flat straight line and the curve of 150R. When the front vehicle was more than 60 m ahead, some detection error occurred due to the degraded white line data accuracy. 5. Conclusion We have developed an automated vehicle system using images (white line) as the principal information. We have indicated that a smooth automatic driving is possible at 60 km/h with the use of a simple control model. Since the steering performance becomes degraded under this control method when the preview distance is made smaller, it is difficult to achieve stable driving with the deviation data from just under the magnetic induction cable, etc. Although preview information is quite effective, it is necessary to improve the image processing speed and develop a steering control method using the road information (such as curve curvature, etc.) to realize stable automated driving at a higher vehicle speed in the future. To achieve the operational application of automated vehicle system, it is imperative for us to develop an environment observation technique that uses the higher image recognition for the purpose of obstacle avoidance and coordination driving with other vehicles. Moreover, to realize a high degree of reliability, it would be necessary for us to have a composite system with the communications system and infrastructure in addition to images. There is a prediction that the automated driving will be realized at the beginning of 21st century in some form or other. In either case, there are many technical problems to solve. Moreover, the automated vehicle system involves a basic problem of whether the responsibility of driving lies on the driver or the vehicle. In this connection, the matter requires thorough discussion so that the social consensus would be reached concerning the application system and the governing law after giving consideration to every possible situation that may arise from the automated driving. On the basis of these matters, we intend to develop more intelligent automated vehicle system in the future. References 1. Yoshimoto: ``Modeling the Driving Behavior of an Automobile Driver,'' Ningen Kogaku (in Japanese), Vol. 18, No. 6, pp 301-305 (1982). 2. E. D. Dickmanns, A. Zapp: ``A Curvature-Based Scheme for Improving Road Vehicle Guidance by Computer Vision,'' SPIE Vol. 727, Mobile Robots, pp 161-168 (1986). 3. Shigematsu, Watanabe, Shima, and Ohnishi: ``Automatic Vehicle Control System Using Driver Model,'' Journal of the Society of Automobile Engineers of Japan, Vol. 45, No. 2, pp 19-24 (1991). 4. Zen, Sakurai, Kobayashi, and Ozawa: ``Analysis of a Road Image as Seen From a Vehicle,'' The Transaction of the Institute of Electronics, Information and Communication Engineers (D), Vol. J71-D, No. 9, pp 1709-1717 (1989). 5. Taniguchi and Hosaka: ``Technical Trends in Autonomous Driving Vehicles,'' Nissan Technical Review, Vol. 28, pp 87-93 (1990).
