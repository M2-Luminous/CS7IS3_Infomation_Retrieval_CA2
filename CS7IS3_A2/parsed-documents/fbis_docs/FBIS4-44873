FBIS4-44873 "jpjst028__l94009" JPRS-JST-94-028L JPRS Science & Technology Japan 23 June 1994 Energy Human Interface Technology Human Interface Technology Speech Recognition Interface for General-Purpose Workstation 94FE0430A Tokyo TOSHIBA REVIEW in Japanese Jan 94 pp 8-11 -- FOR OFFICIAL USE ONLY 94FE0430A Tokyo TOSHIBA REVIEW Japanese CSO [Article by Yoshifumi Nagata and Yoichi Takebayashi] [Text] 1. Foreword The voice is a natural input system for humans, but a massive amount of computation is necessary for speech recognition. It has not been broadly applied because of problems such as the framework to integrate the voice input system with the existing interface not being established. Therefore, based on already developed high precision software speech recognition technology[.sup]1[/] applying high speed algorithms, the problems of integrating the voice input media with existing input systems was resolved by making the speech recognition function the server. The newly developed software speech recognition interface and its applications are discussed here. 2. Speech Recognition in a Workstation 2.1 Making a High Capacity Workstation Since speech recognition processing requires a massive amount of computation, hardware exclusively for speech recognition such as the prior digital signal processor (DSP) was necessary, but the authors used the computation capacity of current general workstations and developed a software speech recognition system operating in real time with high speed algorithms.[.sup]1[/] This system performs high precision speech recognition responding to specific and non-specific speakers based on the composite analog method. Recently, other speech recognition systems with just software for specific speakers have been developed and an environment in which the speech recognition function can be used easily has been prepared. 2.2 Improvement of Human Interface by Voice Input Research on continuous speech recognition is being performed with the objectives of high precision text input by voice and dialogue with the computer, but the recognition capacity for continuous speech is at the level of evaluation in a laboratory environment and the recognition of isolated words is realistic when use in a real environment is considered. Also, in order for voice input to be a standard input system like the keyboard or mouse, the actual method of the speech recognition interface is important along with the recognition capacity, but the framework to integrate voice input with existing interfaces is not established in the multimedia and multitasking environment which is generally used in workstations. Xspeak[.sup]2[/] and CM-SLS (CMU Spoken Language Shell)[.sup]3[/] are being tried for the speech recognition interface with the assumption of multitasking. Xspeak can perform voice input to multitasking in the X-Window environment (X-Window is software developed at MIT.), but has the limitation that feedback of the internal state of the applied program to the recognition equipment is not possible. Also, since CM-SLS is an interface for use of continuous speech recognition equipment by means of a network, examination of real time processing and the form of use on a personal workstation is not sufficient. The above points were considered, software speech recognition technology for which application expansion is easily designed was employed, multitasking was handled by making the speech recognition function a server, and improved quality of the human interface was designed. 3. Software Speech Recognition 3.1 Software Speech Recognition by the Multiple Similarity Method Since the capacity of speech recognition greatly influences the usage state, this system emphasizes high precision with the subject being the recognition of isolated words which can guarantee high recognition capacity. The function for recognizing single word speech consists of the acoustic processor and the multiple similarity operator as shown in Figure 1 (a). It uses the feature vectors attained by the acoustic processor, performs matching by multiple similarities with the recognition dictionary, and output the recognition results. Figure 1. Speech-recognition algorithm using the multiple similarity method. In the high speed algorithm (a), speed is increased by performing the frequency analysis having many operations only in the word intervals. <annot> Key: (a) High speed recognition algorithm, 1. Acoustic processor, 2. Voice, 3. Beginning and ending point detection, 4. Resample frame determination, 5. Frequency analysis, 6. Word feature vector extraction, 7. Similarity operator, 8. Similarity operation, 9. Recognition dictionary, 10. Recognition results, (b) Prior recognition algorithm</graphic> In the acoustic processor, voice input is made at sampling frequency 8 kHz and quantization 8 bits with the telephone band CODEC (coding-decoding) contained in a general workstation. The beginning and ending points of words are detected by the time change of voice energy. Afterwards, it splits the word intervals into equal intervals, determines a sample point along the time axis, performs frequency analysis by a fast Fourier transformation (FFT), levels the frequency analysis results, for example it attains filter bank output, and outputs the time frequency pattern as the word feature vectors. Next, in the similarity operator, it calculates the multiple similarity of the word feature vector and the recognition dictionary shown in the following equation. EQUATION Here, S[.sup](i)[/][X] is the category i similarity value, X is the input word feature vector, M is the axis number, _l_[.sup](i)[/][.sub]m[/] and _l_[.sup](i)[/][.sub]1[/] are the proper values and _P_[.sup](i)[/][.sub]m[/] is the proper vector. The above _P_[.sup](i)[/][.sub]m[/] is found using K-L development from many patterns (word feature vector). The multiple similarity value of each word is found with the software and word recognition is performed. The word recognition discussed above is distinguished by performing high precision recognition processing entirely with software and without using any external hardware. In the prior recognition algorithm according to the multiple similarity method in Figure 1 (b), convention required dedicated hardware such as DSP in order for voice analysis since frequency analysis was performed at all sample points (analysis frame) in the direction of time. Meanwhile, in the high speed algorithm, the amount of computations can be greatly reduced since analysis is performed for only the data resampled at regular intervals in the voice intervals detected at the beginning and ending points as shown in Figure 2. With this high speed algorithm, a speech recognition system with just software can operate in real time in a multitasking environment. Figure 2. Extraction of a word feature vector. The word interval is split into regular intervals, frequency analysis is made only at resample points set on the time axis, and the word feature vectors are extracted. <annot> Key: 1. Power, 2. Threshold value, 3. Start point, 4. End point, 5. Time change of voice power, 6. Time, 7. Frequency, 8. BPF output, 9. Channel, 10. Dimension word feature vector</graphic> 3.2 Improvement of Recognition Capacity by Partial Abstraction Since the recognition of word units used in this system can express the entire and dynamic feature of a word by making a single pattern of the entire word, high recognition capacity is attained for the system in which the subject is a non-specific speaker under noisy conditions. Furthermore, it has the advantage that it can be applied to foreign languages as well just by changing the word dictionary. Also, the word pattern is expressed as several word feature vectors with different dimension numbers from different viewpoints due to partial abstraction.[.sup](4)[/] For example, Figure 3 is the result of extracting several feature vectors of the word ``clear,'' Figure 3 (a) is the expression of the entire word, (b) is the detailed expression of the first half, and (c) is the detailed expression of the second half. With this partial abstraction, the verification of word voice from different viewpoints is performed based on the same multiple similarity; the recognition capacity can be improved by integrated processing since the trend of error recognition is generally different in each expression. In particular, the effect of partial abstraction can be anticipated in the case where word pairs in which part of the word inscription is the same are present in the recognition vocabulary. Figure 3. Example of frequency analysis and word feature vector. The word pattern is illustrated by several feature vectors extracted from the different subject sections at different resolutions. <annot> Key: (a) Total pattern (16x16 dimension), (b) First half pattern (12x32 dimension), (c) Second half pattern (12x32 dimension), 1. Waveform (word = ``clear''), 2. FFT analysis results, 3. Word feature vector</graphic> The capacity for non-specific speaker word recognition of this speech recognition equipment was 98.0% before partial abstraction and 99.2% after introduction in the case of a recognition subject vocabulary of 26 words necessary for DTP system operation. Especially in the case of using the entire pattern of the word, recognition errors were observed in the word pairs of ``grouping'' and ``group canceling'' or ``grouping'' and ``smoothing.'' However, this was greatly improved with the introduction of partial abstraction. Here. the partial abstraction was performed using the pattern of the first and second halves of a 160 dimension word. 4. Mounting the Speech Recognition Interface 4.1 Application of the Client-Server Model The method of interposing the speech recognition driver in Figure 4 (a) and the method of key emulation in which the speech recognition results are converted to key input in Figure 4 (b) are current speech recognition interfaces. Both of these methods do not presuppose usage in a multitasking environment and have the problem that changes to the vocabulary which can be input and changes of the internal state on the applied program side cannot be fed back into the recognition function. The authors resolved these problems by making a server of the speech recognition function based on the client-server model shown in Figure 4 (c). This model separates tasks requiring processing (client) and tasks performing processing (server) and advances processing by performing communication between those. The advantage is that the several applied programs which are the client can have joint resources for speech recognition through the server and the several clients can use the speech recognition function. Figure 4. Configuration of speech recognition interface. The server type (c) can handle multitasking and performs minute processing by the reciprocal communication between client and server. <annot> Key: (a) Driver type, 1. Voice, 2. Speech recognition driver, 3. Applied program, (b) Key emulator type, 4. Speech recognition program, (c) Server type, 5. Speech recognition server</graphic> 4.2 Multimodal Correspondence Speech Recognition Server The trial speech recognition server was mounted on our company's workstation SPARC (SPARC is the registered trademark of SPARC International, Inc. in the US.) using the software speech recognition system discussed above. The speech recognition server usually operates as one process in the multitasking environment and makes notification of the recognition results and recognitions vocabulary by communication with the client. Figure 5 shows an example of usage of the speech recognition server combined with a Window system. Windows A, B, and C are the clients of the speech recognition server and the voice focus designating client B as the transmission destination of the speech recognition results is assigned. For the voice input, the server uses the recognition vocabulary requested by client B, performs recognition processing, and transmits the results to client B. Figure 5. Speech recognition server on a workstation. Multimodal input can be effectively performed by establishing the voice focus and the keyboard focus separately. <annot> Key: 1. Screen display, 2. Voice focus, 3. Keyboard focus, 4. Speech recognition dictionary, 5. Speech recognition server, 6. Acoustic input equipment, 7. Microphone</graphic>; The authors connected the two media of keyboard input and voice input to separate tasks and made possible the control of several applied programs at the same time through separate input channels. So that the user can distinguish the separate input tasks, a task which the user can input is displayed in the center of the screen as in Figure 5 for example. In this figure, the keyboard focus is expressed with the thickened window frame and the voice focus is expressed with the change of window title color. The voice focus is set with the mouse and voice (window name). Thus, multimodal input handling the voice, keyboard, and mouse at the same time can be effectively performed. With the realization of speech recognition function by software only and the supposition of the speech recognition function following the server, the usage of standard speech recognition without depending on hardware became possible. Also, under a multi-window environment, voice input for several applied programs became possible. 5. Application of Speech Recognition Interface Several application programs have been tried as clients with the objective of evaluating the speech recognition server. Of those, the application to the DTP system is discussed as an example. The DTP system has a screen as in Figure 6 (a) and performs editing of text and images. The DTP control is performed by converting and transmitting the speech recognition results to keyboard commands with the control program shown in Figure 6 (b). Keyboard commands use the API (Application Program Interface) provided by the DTP system. The recognition vocabulary uses the menu tier of the DTP system and is limited and the recognition capacity can remain high. The route of the menu tier is called the ``top level;'' words from the top level are spoken and the commands are executed by going through the menu. At each movement in the menu tier, the menu item in the window and the current position in the menu tier are expressed in the form of a path and presented to the user. The operation target is selected with the mouse and the operations such as ``cut,'' ``paste,'' and ``up/down reverse'' for the target are performed by voice. Since these two media can share roles in a natural form, complex mouse operations during text editing and figure editing are reduced and an interface with good usage conditions can be realized. 6. Conclusion A standard speech recognition interface for a workstation multimedia/multitasking environment was developed. This speech recognition interface can handle multimodal input including keyboard, mouse, and voice input by making a server of the speech recognition function. Also, with the high precision software speech recognition technology performing processing in real time, a wide range of applications which do not rely on hardware are possible. In the future, research is planned with the objective of establishing a human interface with better usage conditions and using voice media. Footnotes 1. Y. Nagata, et al: Development of the speech recognition function in a workstation, Denshi Joho Tsushin Gakkai Kenkyukaishiryo, HC-9119, pp 63-70 (1991). 2. C. Schmandt et al: Augmenting a window system with speech input, Computer, No. 23, pp 50-58 (1990). 3. A. Rudnicky, et al: Spoken language recognition in an office management domain, Proc. Int. Conf. Acoust. Speech & Signal Process, S14.7, pp 829-832 (1991). 4. Y. Takebayashi, et al: Keyboard spotting by partial abstraction of word feature vectors, Denshi Joho Tsushin Gakkai Onsei Kenkyukai, SP91-104 (1991).
